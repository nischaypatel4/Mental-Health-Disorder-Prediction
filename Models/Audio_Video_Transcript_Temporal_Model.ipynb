{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX74--PYeN1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797c3bef-3172-4ac4-a940-e6605e063b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "summary_features = pd.read_csv('/content/participant_summary_new.csv')\n",
        "#summary_features = pd.read_csv('/content/drive/My Drive/full_participant_summary.csv')\n",
        "\n",
        "print(summary_features.shape)"
      ],
      "metadata": {
        "id": "t9oM3pLbeZM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d150d6d-bdc5-4c96-edac-181ac8214ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(275, 1014)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_names = []\n",
        "for i in range(1, 50):  # f1 to f49\n",
        "    for stat in ['mean', 'std', 'min', 'max']:\n",
        "        new_names.append(f'OpenFace_f{i}_{stat}')\n",
        "\n",
        "# Apply renaming to your dataframe\n",
        "column_indices_to_rename = range(802, 998)  # Inclusive of 802 to 997\n",
        "summary_features.columns.values[list(column_indices_to_rename)] = new_names"
      ],
      "metadata": {
        "id": "onic0RhXTq5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_features = summary_features.loc[:, (summary_features != 0).any(axis=0)]"
      ],
      "metadata": {
        "id": "PdaYuuyISmea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your label CSV\n",
        "df = pd.read_csv('/content/GroundTruth Table.csv')\n",
        "\n",
        "# Define labels\n",
        "labels = ['PTSD_Label',\t'Depression_Label',\t'Appetite_Label',\t'Agency_Label',\t'Anxiety_Label',\t'Sleep_Label']\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = range(len(labels))\n",
        "zeros = [df[label].value_counts().get(0, 0) for label in labels]\n",
        "ones = [df[label].value_counts().get(1, 0) for label in labels]\n",
        "\n",
        "bar_width = 0.35\n",
        "ax.bar([i - bar_width/2 for i in x], zeros, width=bar_width, label='Class 0')\n",
        "ax.bar([i + bar_width/2 for i in x], ones, width=bar_width, label='Class 1')\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Class Distribution per Label')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DJBMTXhEA-7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "a102ecf3-264b-4dd6-e79a-8ed96fbcd198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# Separate features (summary_features) and the identified labels\n",
        "X = summary_features\n",
        "y = df[labels[0]] #i =0, 1, 2, 3, 4, 5 for diffrent targets\n",
        "\n",
        "# Initialize StratifiedKFold with 5 splits\n",
        "n_splits = 5  # Set to 5 for 5 iterations/folds\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "UDl-FWhxq94O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORSxAkMluoxQ",
        "outputId": "380f3b7c-11ff-4a58-d57a-f4c3ce32ca29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(275,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pns81jeeuYff",
        "outputId": "f0ff1e63-395d-49cf-df70-1d870bd74c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(275, 806)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = summary_features.copy()\n",
        "\n",
        "# Label encoding for categoricals\n",
        "for colname in X.select_dtypes(\"object\"):\n",
        "    X[colname], _ = X[colname].factorize()\n",
        "\n",
        "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
        "discrete_features = X.dtypes == int"
      ],
      "metadata": {
        "id": "0LCpJKv9pJw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discrete_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "Q0HalCqCuc-_",
        "outputId": "a877f48b-1ad7-475c-f117-8553117e533e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "participant_id            True\n",
              "eGeMAPS_f1_mean          False\n",
              "eGeMAPS_f1_std           False\n",
              "eGeMAPS_f1_max           False\n",
              "eGeMAPS_f2_mean          False\n",
              "                         ...  \n",
              "depression_word_count     True\n",
              "anxiety_word_count        True\n",
              "stress_word_count         True\n",
              "ptsd_word_count           True\n",
              "harm_word_count           True\n",
              "Length: 806, dtype: bool"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>participant_id</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eGeMAPS_f1_mean</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eGeMAPS_f1_std</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eGeMAPS_f1_max</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eGeMAPS_f2_mean</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>depression_word_count</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anxiety_word_count</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stress_word_count</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ptsd_word_count</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>harm_word_count</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>806 rows  1 columns</p>\n",
              "</div><br><label><b>dtype:</b> bool</label>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P47sE-ZsMqB",
        "outputId": "22b024f1-0d10-4462-f2a1-6e352b2af314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(275, 806)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "u0QS2Go7sOWk",
        "outputId": "7d848cf8-bae9-4ea1-c2f0-ed2df6642170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   participant_id  eGeMAPS_f1_mean  eGeMAPS_f1_std  eGeMAPS_f1_max  \\\n",
              "0             300         0.339651        0.412770        1.633468   \n",
              "1             301         0.417114        0.383984        1.462398   \n",
              "2             302         0.431654        0.399046        1.414973   \n",
              "3             303         0.688838        0.391137        1.544068   \n",
              "4             304         0.594773        0.377326        1.477121   \n",
              "\n",
              "   eGeMAPS_f2_mean  eGeMAPS_f2_std  eGeMAPS_f2_max  eGeMAPS_f3_mean  \\\n",
              "0         0.309486        0.527131        2.008600         0.098401   \n",
              "1         0.508682        0.565141        1.963788         0.048905   \n",
              "2         0.008801        0.083450        1.041393         0.070662   \n",
              "3         1.028125        0.639955        2.037427         0.041777   \n",
              "4         0.658804        0.659056        1.977724         0.043229   \n",
              "\n",
              "   eGeMAPS_f3_std  eGeMAPS_f3_max  ...  lexical_diversity  \\\n",
              "0        0.224203        1.079181  ...           0.336842   \n",
              "1        0.167753        1.000000  ...           0.238649   \n",
              "2        0.204610        0.954242  ...           0.330756   \n",
              "3        0.147273        0.954242  ...           0.211736   \n",
              "4        0.154882        0.903090  ...           0.269373   \n",
              "\n",
              "   first_person_pronoun_count  negation_count  sleep_word_count  \\\n",
              "0                          62               9                 1   \n",
              "1                         150              23                 1   \n",
              "2                          81              13                 1   \n",
              "3                         164              16                 6   \n",
              "4                         120              21                 4   \n",
              "\n",
              "   appetite_word_count  depression_word_count  anxiety_word_count  \\\n",
              "0                    0                      2                   1   \n",
              "1                    0                      3                   1   \n",
              "2                    2                      2                   2   \n",
              "3                    1                      3                   2   \n",
              "4                    0                      1                   1   \n",
              "\n",
              "   stress_word_count  ptsd_word_count  harm_word_count  \n",
              "0                  0                1                2  \n",
              "1                  1                3                2  \n",
              "2                  0                1                2  \n",
              "3                  1                3                1  \n",
              "4                  0                2                1  \n",
              "\n",
              "[5 rows x 806 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef5bfe62-48dd-490f-b723-515b4ac0ed04\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant_id</th>\n",
              "      <th>eGeMAPS_f1_mean</th>\n",
              "      <th>eGeMAPS_f1_std</th>\n",
              "      <th>eGeMAPS_f1_max</th>\n",
              "      <th>eGeMAPS_f2_mean</th>\n",
              "      <th>eGeMAPS_f2_std</th>\n",
              "      <th>eGeMAPS_f2_max</th>\n",
              "      <th>eGeMAPS_f3_mean</th>\n",
              "      <th>eGeMAPS_f3_std</th>\n",
              "      <th>eGeMAPS_f3_max</th>\n",
              "      <th>...</th>\n",
              "      <th>lexical_diversity</th>\n",
              "      <th>first_person_pronoun_count</th>\n",
              "      <th>negation_count</th>\n",
              "      <th>sleep_word_count</th>\n",
              "      <th>appetite_word_count</th>\n",
              "      <th>depression_word_count</th>\n",
              "      <th>anxiety_word_count</th>\n",
              "      <th>stress_word_count</th>\n",
              "      <th>ptsd_word_count</th>\n",
              "      <th>harm_word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>300</td>\n",
              "      <td>0.339651</td>\n",
              "      <td>0.412770</td>\n",
              "      <td>1.633468</td>\n",
              "      <td>0.309486</td>\n",
              "      <td>0.527131</td>\n",
              "      <td>2.008600</td>\n",
              "      <td>0.098401</td>\n",
              "      <td>0.224203</td>\n",
              "      <td>1.079181</td>\n",
              "      <td>...</td>\n",
              "      <td>0.336842</td>\n",
              "      <td>62</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>301</td>\n",
              "      <td>0.417114</td>\n",
              "      <td>0.383984</td>\n",
              "      <td>1.462398</td>\n",
              "      <td>0.508682</td>\n",
              "      <td>0.565141</td>\n",
              "      <td>1.963788</td>\n",
              "      <td>0.048905</td>\n",
              "      <td>0.167753</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.238649</td>\n",
              "      <td>150</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>302</td>\n",
              "      <td>0.431654</td>\n",
              "      <td>0.399046</td>\n",
              "      <td>1.414973</td>\n",
              "      <td>0.008801</td>\n",
              "      <td>0.083450</td>\n",
              "      <td>1.041393</td>\n",
              "      <td>0.070662</td>\n",
              "      <td>0.204610</td>\n",
              "      <td>0.954242</td>\n",
              "      <td>...</td>\n",
              "      <td>0.330756</td>\n",
              "      <td>81</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>303</td>\n",
              "      <td>0.688838</td>\n",
              "      <td>0.391137</td>\n",
              "      <td>1.544068</td>\n",
              "      <td>1.028125</td>\n",
              "      <td>0.639955</td>\n",
              "      <td>2.037427</td>\n",
              "      <td>0.041777</td>\n",
              "      <td>0.147273</td>\n",
              "      <td>0.954242</td>\n",
              "      <td>...</td>\n",
              "      <td>0.211736</td>\n",
              "      <td>164</td>\n",
              "      <td>16</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>304</td>\n",
              "      <td>0.594773</td>\n",
              "      <td>0.377326</td>\n",
              "      <td>1.477121</td>\n",
              "      <td>0.658804</td>\n",
              "      <td>0.659056</td>\n",
              "      <td>1.977724</td>\n",
              "      <td>0.043229</td>\n",
              "      <td>0.154882</td>\n",
              "      <td>0.903090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269373</td>\n",
              "      <td>120</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  806 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef5bfe62-48dd-490f-b723-515b4ac0ed04')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ef5bfe62-48dd-490f-b723-515b4ac0ed04 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ef5bfe62-48dd-490f-b723-515b4ac0ed04');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b49e95d2-e60d-45ff-954e-e849022f2751\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b49e95d2-e60d-45ff-954e-e849022f2751')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b49e95d2-e60d-45ff-954e-e849022f2751 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CV8hWGOuie2",
        "outputId": "2b715e63-874e-4e00-ee49-9efff066dd75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(275,)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n"
      ],
      "metadata": {
        "id": "W1DjRVdLpbBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_mi_scores(X, y, discrete_features):\n",
        "    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n",
        "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
        "\n",
        "    return mi_scores\n",
        "\n",
        "# mi_scores = make_mi_scores(X, y, discrete_features)\n",
        "# mi_scores  # show a few features with their MI scores"
      ],
      "metadata": {
        "id": "hngO45Nks7SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PP5UNx7n28yB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model for Temporal Features (CNN+BILSTM)"
      ],
      "metadata": {
        "id": "IJGP_U_GI-Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, BatchNormalization, GlobalMaxPooling1D, Dropout\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "iAvnAvWgDL_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CHUNKS = 87"
      ],
      "metadata": {
        "id": "hAeHmeEeC8br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def apply_pca(X_all):\n",
        "  pca = PCA(n_components=40)  # or 50, depending on desired reduction\n",
        "  X_embed_scaled = StandardScaler().fit_transform(X_all.iloc[:,-384:])\n",
        "\n",
        "  X_reduced = pca.fit_transform(X_embed_scaled)\n",
        "\n",
        "  # Explained variance ratio for each component\n",
        "  explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "  # Cumulative explained variance\n",
        "  cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "  # Replacing SBERT embeddings with PCs\n",
        "  # Step 1: Drop the last 384 columns\n",
        "  X_all = X_all.iloc[:, :-384]\n",
        "\n",
        "  # Step 2: Convert X_reduced (NumPy array) to DataFrame\n",
        "  X_reduced_df = pd.DataFrame(X_reduced, index=X_all.index)\n",
        "\n",
        "  # Optional: rename columns\n",
        "  X_reduced_df.columns = [f'embed_{i}' for i in range(X_reduced_df.shape[1])]\n",
        "\n",
        "  # Step 3: Concatenate\n",
        "  X_all = pd.concat([X_all, X_reduced_df], axis=1)\n",
        "  return X_all"
      ],
      "metadata": {
        "id": "5U0CN2UcP6on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dropout, Bidirectional, LSTM, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import re\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 20:\n",
        "        return lr\n",
        "    elif epoch == 20:\n",
        "        return lr * 0.1\n",
        "    elif epoch == 40:\n",
        "        return lr * 0.1\n",
        "    else:\n",
        "        return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler, verbose=1)\n",
        "\n",
        "# Wrap your model in a function\n",
        "def build_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_model_ragged(input_dim):\n",
        "    inp = Input(shape=(None, input_dim))  # variable-length support\n",
        "\n",
        "    x = Conv1D(4, kernel_size=5, activation='relu')(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # x = SimpleRNN(4, return_sequences=True)(x)\n",
        "    x = Bidirectional(LSTM(4, return_sequences=True))(x)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "\n",
        "    x = Dense(4, activation='tanh')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    out = Dense(2, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def remove_padding(X):\n",
        "    sequences = []\n",
        "    for x in X:\n",
        "        # Find non-zero time steps (axis=1 because time is axis 0 in x)\n",
        "        non_zero_mask = np.any(x != 0, axis=1)\n",
        "        trimmed = x[non_zero_mask]\n",
        "        sequences.append(trimmed)\n",
        "    return sequences\n"
      ],
      "metadata": {
        "id": "DvKDq2zmEN5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEMPORAL FEATURES MODEL"
      ],
      "metadata": {
        "id": "ncS74BK8J5T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics_dl(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    sens = tp / (tp + fn) if (tp + fn) else 0\n",
        "    spec = tn / (tn + fp) if (tn + fp) else 0\n",
        "    return round(acc, 4), round(sens, 4), round(spec, 4)\n",
        "\n",
        "# Result holder: label -> [acc, sens, spec]\n",
        "results_dl = {}\n",
        "df = pd.read_csv('/content/GroundTruth Table.csv')\n",
        "MAX_CHUNKS = 87\n",
        "# Replace these with your actual label list and data (X_all_label, y_all_label)\n",
        "labels = ['Appetite_Label', 'Anxiety_Label', 'Sleep_Label', 'Agency_Label', 'Depression_Label', 'PTSD_Label']\n",
        "extracted_features = []\n",
        "extra_features = [f\"feature_{i}\" for i in range(249, 633)]\n",
        "extracted_features.extend(extra_features)\n",
        "\n",
        "X_all_label = pd.read_csv('/content/All_features.csv', usecols = extracted_features)  # shape: (samples, MAX_CHUNKS, features) for this label\n",
        "print(X_all_label.shape)\n",
        "X_all_label = apply_pca(X_all_label)\n",
        "\n",
        "for label in labels:\n",
        "    # X = summary_features.copy()\n",
        "    # y = df[label]\n",
        "\n",
        "    # discrete_features = [False] * X.shape[1]\n",
        "    # mi_scores = make_mi_scores(X, y, discrete_features)\n",
        "    # filtered_scores = mi_scores[mi_scores > 0]\n",
        "    # filtered_X = X[filtered_scores.index]\n",
        "\n",
        "    # # Drop highly correlated features\n",
        "    # corr_matrix = filtered_X.corr().abs()\n",
        "    # to_drop = {\n",
        "    #     col2 if filtered_scores[col1] >= filtered_scores[col2] else col1\n",
        "    #     for i, col1 in enumerate(corr_matrix.columns)\n",
        "    #     for j, col2 in enumerate(corr_matrix.columns)\n",
        "    #     if i < j and corr_matrix.loc[col1, col2] > 0.8\n",
        "    # }\n",
        "\n",
        "    # final_X = filtered_X.drop(columns=to_drop)\n",
        "    # final_scores = filtered_scores.drop(labels=to_drop)\n",
        "\n",
        "    # # Select top 20% features\n",
        "    # top_features = final_scores.sort_values(ascending=False).head(int(len(final_scores) * 0.2))\n",
        "    # top_features_list = top_features.index.tolist()\n",
        "\n",
        "    # extracted_features = []\n",
        "\n",
        "    # for feature in top_features_list:\n",
        "    #     match = re.match(r'^(MFCC|eGeMAPS|FAU)_f(\\d+)_((?:mean)|(?:std)|(?:min)|(?:max))$', feature)\n",
        "    #     if match:\n",
        "    #         category, number, _ = match.groups()\n",
        "    #         number = int(number)\n",
        "    #         if category == \"MFCC\":\n",
        "    #             mapped = f\"feature_{number-1}\"\n",
        "    #         elif category == \"eGeMAPS\":\n",
        "    #             mapped = f\"feature_{number + 100-1}\"\n",
        "    #         # elif category == \"FAU\":\n",
        "    #         #     mapped = f\"feature_{number + 200-1}\"\n",
        "    #         extracted_features.append(mapped)\n",
        "    # # extra_features = [f\"feature_{i}\" for i in range(249, 633)]\n",
        "    # # extracted_features.extend(extra_features)\n",
        "\n",
        "\n",
        "    print(f\"\\n=== Deep Learning on {label} ===\")\n",
        "    y_all_label = df[label] # binary labels for this label\n",
        "    X_all_label = np.array(X_all_label)\n",
        "    print(X_all_label.shape)\n",
        "    X_all_label = X_all_label.reshape(275, 87, -1)\n",
        "    print(X_all_label.shape)\n",
        "\n",
        "\n",
        "    fold_metrics = []\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    for train_index, test_index in kf.split(X_all_label, y_all_label):\n",
        "        X_train, X_test = X_all_label[train_index], X_all_label[test_index]\n",
        "        y_train, y_test = y_all_label[train_index], y_all_label[test_index]\n",
        "\n",
        "        X_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "        ros = RandomOverSampler()\n",
        "        X_resampled, y_resampled = ros.fit_resample(X_flat, y_train)\n",
        "        X_train = X_resampled.reshape((-1, 87, X_train.shape[2]))\n",
        "        y_train = to_categorical(y_resampled)\n",
        "\n",
        "        X_train = X_train[:,:,:]\n",
        "        X_test = X_test[:,:,:]\n",
        "\n",
        "        X_train = remove_padding(X_train)\n",
        "        X_test = remove_padding(X_test)\n",
        "\n",
        "        X_train = tf.ragged.constant(X_train, dtype=tf.float32)\n",
        "        X_test = tf.ragged.constant(X_test, dtype=tf.float32)\n",
        "        y_test = to_categorical(y_test)\n",
        "\n",
        "        X_train = X_train.to_tensor()\n",
        "        X_test = X_test.to_tensor()\n",
        "\n",
        "        model = build_model_ragged(input_dim=X_train.shape[2])\n",
        "        model.fit(X_train, y_train, batch_size=16, epochs=15, validation_data=(X_test, y_test), callbacks=[lr_scheduler], verbose=1, shuffle=True)\n",
        "\n",
        "        y_pred_prob = model.predict(X_test)\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "        y_test = np.argmax(y_test, axis=1)\n",
        "        acc, sens, spec = compute_metrics_dl(y_test, y_pred)\n",
        "        fold_metrics.append((acc, sens, spec))\n",
        "\n",
        "    avg_metrics = np.mean(fold_metrics, axis=0)\n",
        "    results_dl[label] = list(map(lambda x: round(x, 4), avg_metrics))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZUQYA_3obk-",
        "outputId": "d68e57d1-1869-4a12-8865-cdb274f744bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23925, 384)\n",
            "\n",
            "=== Deep Learning on Appetite_Label ===\n",
            "(23925, 40)\n",
            "(275, 87, 40)\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 144ms/step - accuracy: 0.5516 - loss: 0.7220 - val_accuracy: 0.7455 - val_loss: 0.5496 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.6338 - loss: 0.6501 - val_accuracy: 0.8000 - val_loss: 0.5322 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7273 - loss: 0.6046 - val_accuracy: 0.7636 - val_loss: 0.4840 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7415 - loss: 0.5832 - val_accuracy: 0.8182 - val_loss: 0.4839 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7804 - loss: 0.5057 - val_accuracy: 0.7455 - val_loss: 0.5335 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8230 - loss: 0.4588 - val_accuracy: 0.7636 - val_loss: 0.5167 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7710 - loss: 0.4992 - val_accuracy: 0.6727 - val_loss: 0.7057 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8435 - loss: 0.4085 - val_accuracy: 0.6909 - val_loss: 0.7702 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.8137 - loss: 0.4222 - val_accuracy: 0.7273 - val_loss: 0.7133 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.8297 - loss: 0.4060 - val_accuracy: 0.7455 - val_loss: 0.6691 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8392 - loss: 0.3424 - val_accuracy: 0.7455 - val_loss: 0.7452 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9049 - loss: 0.3043 - val_accuracy: 0.7455 - val_loss: 0.6968 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8930 - loss: 0.3074 - val_accuracy: 0.7636 - val_loss: 0.8282 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8973 - loss: 0.2600 - val_accuracy: 0.7455 - val_loss: 0.8479 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9288 - loss: 0.2393 - val_accuracy: 0.7091 - val_loss: 0.9034 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 579ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.5479 - loss: 0.7061 - val_accuracy: 0.5273 - val_loss: 0.6681 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.5808 - loss: 0.6631 - val_accuracy: 0.7091 - val_loss: 0.6002 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7035 - loss: 0.5827 - val_accuracy: 0.7273 - val_loss: 0.5532 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.7658 - loss: 0.5284 - val_accuracy: 0.7455 - val_loss: 0.5328 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.7901 - loss: 0.4356 - val_accuracy: 0.7091 - val_loss: 0.5377 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8261 - loss: 0.4090 - val_accuracy: 0.7455 - val_loss: 0.5499 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8546 - loss: 0.3754 - val_accuracy: 0.7818 - val_loss: 0.5220 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8362 - loss: 0.3735 - val_accuracy: 0.7455 - val_loss: 0.4666 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8668 - loss: 0.3384 - val_accuracy: 0.7273 - val_loss: 0.5185 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8447 - loss: 0.3728 - val_accuracy: 0.7273 - val_loss: 0.4979 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8887 - loss: 0.2829 - val_accuracy: 0.7818 - val_loss: 0.5370 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8210 - loss: 0.4650 - val_accuracy: 0.7273 - val_loss: 0.5331 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.8923 - loss: 0.2866 - val_accuracy: 0.7455 - val_loss: 0.4570 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9232 - loss: 0.2290 - val_accuracy: 0.7091 - val_loss: 0.5554 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9162 - loss: 0.2602 - val_accuracy: 0.7273 - val_loss: 0.5427 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 400ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 132ms/step - accuracy: 0.4863 - loss: 0.7386 - val_accuracy: 0.2545 - val_loss: 0.7273 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.6024 - loss: 0.6557 - val_accuracy: 0.2545 - val_loss: 0.8026 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.6148 - loss: 0.6416 - val_accuracy: 0.4182 - val_loss: 0.7521 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7423 - loss: 0.5840 - val_accuracy: 0.4000 - val_loss: 0.8592 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7884 - loss: 0.5134 - val_accuracy: 0.4182 - val_loss: 0.8976 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.7397 - loss: 0.5207 - val_accuracy: 0.4182 - val_loss: 0.8800 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8191 - loss: 0.4545 - val_accuracy: 0.5091 - val_loss: 0.8038 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.8296 - loss: 0.4367 - val_accuracy: 0.5455 - val_loss: 0.7961 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.8777 - loss: 0.3367 - val_accuracy: 0.5091 - val_loss: 0.9124 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8661 - loss: 0.3746 - val_accuracy: 0.5636 - val_loss: 0.8930 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8897 - loss: 0.3532 - val_accuracy: 0.5273 - val_loss: 1.1143 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8646 - loss: 0.3338 - val_accuracy: 0.4545 - val_loss: 1.1731 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8211 - loss: 0.3564 - val_accuracy: 0.5455 - val_loss: 1.0781 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8747 - loss: 0.3058 - val_accuracy: 0.6000 - val_loss: 1.0871 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8864 - loss: 0.3375 - val_accuracy: 0.6182 - val_loss: 1.0976 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - accuracy: 0.5233 - loss: 0.7549 - val_accuracy: 0.5273 - val_loss: 0.6868 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5399 - loss: 0.7030 - val_accuracy: 0.4727 - val_loss: 0.6826 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.4942 - loss: 0.6925 - val_accuracy: 0.4364 - val_loss: 0.6891 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.5637 - loss: 0.6832 - val_accuracy: 0.4909 - val_loss: 0.6956 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.5892 - loss: 0.6720 - val_accuracy: 0.7091 - val_loss: 0.6178 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.7519 - loss: 0.6026 - val_accuracy: 0.6909 - val_loss: 0.6189 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.7601 - loss: 0.5348 - val_accuracy: 0.7455 - val_loss: 0.5394 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8003 - loss: 0.4620 - val_accuracy: 0.7273 - val_loss: 0.6035 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8321 - loss: 0.3996 - val_accuracy: 0.7273 - val_loss: 0.6872 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8295 - loss: 0.3989 - val_accuracy: 0.7091 - val_loss: 0.8302 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8492 - loss: 0.3153 - val_accuracy: 0.6909 - val_loss: 0.8595 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8859 - loss: 0.3162 - val_accuracy: 0.7091 - val_loss: 0.8665 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8656 - loss: 0.3102 - val_accuracy: 0.7091 - val_loss: 0.8908 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8467 - loss: 0.3654 - val_accuracy: 0.7091 - val_loss: 0.8867 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8598 - loss: 0.3416 - val_accuracy: 0.7273 - val_loss: 0.8816 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 412ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.5830 - loss: 0.6880 - val_accuracy: 0.5273 - val_loss: 0.6826 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.5505 - loss: 0.6844 - val_accuracy: 0.3091 - val_loss: 0.8358 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.5638 - loss: 0.6755 - val_accuracy: 0.3636 - val_loss: 0.7879 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.6588 - loss: 0.6226 - val_accuracy: 0.3818 - val_loss: 0.8178 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6951 - loss: 0.5936 - val_accuracy: 0.4182 - val_loss: 0.8409 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7572 - loss: 0.5130 - val_accuracy: 0.5273 - val_loss: 0.8005 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8070 - loss: 0.4309 - val_accuracy: 0.5636 - val_loss: 0.7391 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8032 - loss: 0.4343 - val_accuracy: 0.6000 - val_loss: 0.7765 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8595 - loss: 0.4057 - val_accuracy: 0.5818 - val_loss: 0.9074 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8132 - loss: 0.4235 - val_accuracy: 0.6364 - val_loss: 0.7942 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8493 - loss: 0.3844 - val_accuracy: 0.4727 - val_loss: 1.0717 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8442 - loss: 0.3317 - val_accuracy: 0.5273 - val_loss: 1.1430 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.8485 - loss: 0.3336 - val_accuracy: 0.5091 - val_loss: 1.3266 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.8938 - loss: 0.2485 - val_accuracy: 0.5455 - val_loss: 1.2240 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8642 - loss: 0.2983 - val_accuracy: 0.5091 - val_loss: 1.3397 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 414ms/step\n",
            "\n",
            "=== Deep Learning on Anxiety_Label ===\n",
            "(275, 87, 40)\n",
            "(275, 87, 40)\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 132ms/step - accuracy: 0.5606 - loss: 0.7800 - val_accuracy: 0.6000 - val_loss: 0.6748 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.4838 - loss: 0.7297 - val_accuracy: 0.5818 - val_loss: 0.6926 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5599 - loss: 0.6716 - val_accuracy: 0.5091 - val_loss: 0.7046 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6216 - loss: 0.6454 - val_accuracy: 0.4545 - val_loss: 0.7132 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.5752 - loss: 0.6543 - val_accuracy: 0.5455 - val_loss: 0.7198 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6419 - loss: 0.6356 - val_accuracy: 0.5636 - val_loss: 0.6456 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7520 - loss: 0.5164 - val_accuracy: 0.6182 - val_loss: 0.6435 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7765 - loss: 0.4643 - val_accuracy: 0.6364 - val_loss: 0.6743 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7780 - loss: 0.4487 - val_accuracy: 0.6727 - val_loss: 0.6007 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.8220 - loss: 0.4691 - val_accuracy: 0.6364 - val_loss: 0.6560 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8276 - loss: 0.4005 - val_accuracy: 0.6364 - val_loss: 0.6468 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.8520 - loss: 0.4062 - val_accuracy: 0.6909 - val_loss: 0.6462 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8402 - loss: 0.3956 - val_accuracy: 0.6727 - val_loss: 0.6212 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8440 - loss: 0.3732 - val_accuracy: 0.7091 - val_loss: 0.5786 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8437 - loss: 0.3443 - val_accuracy: 0.7455 - val_loss: 0.5720 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.5309 - loss: 0.7131 - val_accuracy: 0.5091 - val_loss: 0.6977 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6236 - loss: 0.6741 - val_accuracy: 0.4909 - val_loss: 0.7125 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6596 - loss: 0.6562 - val_accuracy: 0.4364 - val_loss: 0.7064 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.5178 - loss: 0.6942 - val_accuracy: 0.5818 - val_loss: 0.6972 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5810 - loss: 0.6511 - val_accuracy: 0.6182 - val_loss: 0.6819 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6223 - loss: 0.6455 - val_accuracy: 0.5636 - val_loss: 0.6737 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6900 - loss: 0.6095 - val_accuracy: 0.5455 - val_loss: 0.6970 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6131 - loss: 0.6324 - val_accuracy: 0.5818 - val_loss: 0.7091 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8107 - loss: 0.5172 - val_accuracy: 0.5636 - val_loss: 0.7471 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.7256 - loss: 0.5185 - val_accuracy: 0.5455 - val_loss: 0.7572 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.7362 - loss: 0.5101 - val_accuracy: 0.5273 - val_loss: 0.7755 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.7648 - loss: 0.5206 - val_accuracy: 0.5818 - val_loss: 0.7456 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7779 - loss: 0.4842 - val_accuracy: 0.5455 - val_loss: 0.7667 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8055 - loss: 0.4529 - val_accuracy: 0.6182 - val_loss: 0.6790 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8709 - loss: 0.3763 - val_accuracy: 0.5818 - val_loss: 0.7405 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 136ms/step - accuracy: 0.5612 - loss: 0.7689 - val_accuracy: 0.4000 - val_loss: 0.7201 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5552 - loss: 0.7138 - val_accuracy: 0.5091 - val_loss: 0.6864 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5173 - loss: 0.6755 - val_accuracy: 0.5273 - val_loss: 0.6821 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6501 - loss: 0.6402 - val_accuracy: 0.5455 - val_loss: 0.6869 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6676 - loss: 0.6296 - val_accuracy: 0.5636 - val_loss: 0.6723 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.5980 - loss: 0.6354 - val_accuracy: 0.5455 - val_loss: 0.7191 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6989 - loss: 0.5732 - val_accuracy: 0.5455 - val_loss: 0.7279 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6959 - loss: 0.5882 - val_accuracy: 0.6364 - val_loss: 0.6963 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7674 - loss: 0.5231 - val_accuracy: 0.6545 - val_loss: 0.7272 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7549 - loss: 0.5184 - val_accuracy: 0.6545 - val_loss: 0.7100 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.7920 - loss: 0.4663 - val_accuracy: 0.6000 - val_loss: 0.7281 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7786 - loss: 0.5040 - val_accuracy: 0.6000 - val_loss: 0.7639 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7853 - loss: 0.4699 - val_accuracy: 0.6364 - val_loss: 0.7670 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8167 - loss: 0.4453 - val_accuracy: 0.6364 - val_loss: 0.7882 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8149 - loss: 0.3952 - val_accuracy: 0.6545 - val_loss: 0.8208 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 441ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 118ms/step - accuracy: 0.4750 - loss: 0.7458 - val_accuracy: 0.4909 - val_loss: 0.7043 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.4977 - loss: 0.7050 - val_accuracy: 0.5091 - val_loss: 0.6842 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.6623 - loss: 0.6591 - val_accuracy: 0.6000 - val_loss: 0.6671 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.6404 - loss: 0.6494 - val_accuracy: 0.6364 - val_loss: 0.6509 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.6486 - loss: 0.6380 - val_accuracy: 0.6182 - val_loss: 0.6392 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.6565 - loss: 0.6141 - val_accuracy: 0.6182 - val_loss: 0.6714 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.7366 - loss: 0.5785 - val_accuracy: 0.6364 - val_loss: 0.6794 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.7329 - loss: 0.5653 - val_accuracy: 0.6727 - val_loss: 0.6281 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7939 - loss: 0.5125 - val_accuracy: 0.6364 - val_loss: 0.6876 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.7859 - loss: 0.5227 - val_accuracy: 0.6364 - val_loss: 0.7234 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7943 - loss: 0.4686 - val_accuracy: 0.5636 - val_loss: 0.7885 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.7924 - loss: 0.4596 - val_accuracy: 0.5636 - val_loss: 0.8337 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8012 - loss: 0.4479 - val_accuracy: 0.6000 - val_loss: 0.7603 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8537 - loss: 0.4170 - val_accuracy: 0.6545 - val_loss: 0.7622 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.7843 - loss: 0.4887 - val_accuracy: 0.6000 - val_loss: 0.8794 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 433ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 165ms/step - accuracy: 0.5207 - loss: 0.7008 - val_accuracy: 0.4909 - val_loss: 0.6848 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.5525 - loss: 0.6761 - val_accuracy: 0.6182 - val_loss: 0.6627 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.6138 - loss: 0.6512 - val_accuracy: 0.6727 - val_loss: 0.6363 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.6923 - loss: 0.5949 - val_accuracy: 0.6909 - val_loss: 0.6131 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7433 - loss: 0.5563 - val_accuracy: 0.6545 - val_loss: 0.6018 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8069 - loss: 0.4941 - val_accuracy: 0.6000 - val_loss: 0.6649 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7890 - loss: 0.5058 - val_accuracy: 0.7091 - val_loss: 0.6605 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.7791 - loss: 0.5131 - val_accuracy: 0.6727 - val_loss: 0.6772 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8045 - loss: 0.4526 - val_accuracy: 0.6727 - val_loss: 0.6668 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8062 - loss: 0.4436 - val_accuracy: 0.6909 - val_loss: 0.7164 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.8705 - loss: 0.3603 - val_accuracy: 0.6545 - val_loss: 0.7339 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.7620 - loss: 0.5031 - val_accuracy: 0.6545 - val_loss: 0.7558 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8596 - loss: 0.3441 - val_accuracy: 0.7455 - val_loss: 0.7096 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8293 - loss: 0.3973 - val_accuracy: 0.7091 - val_loss: 0.6750 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9029 - loss: 0.2672 - val_accuracy: 0.7091 - val_loss: 0.7717 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436ms/step\n",
            "\n",
            "=== Deep Learning on Sleep_Label ===\n",
            "(275, 87, 40)\n",
            "(275, 87, 40)\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 90ms/step - accuracy: 0.5487 - loss: 0.7192 - val_accuracy: 0.5455 - val_loss: 0.6947 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.5617 - loss: 0.6768 - val_accuracy: 0.6364 - val_loss: 0.6362 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.6122 - loss: 0.6569 - val_accuracy: 0.6182 - val_loss: 0.6370 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6706 - loss: 0.6247 - val_accuracy: 0.6182 - val_loss: 0.6321 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7035 - loss: 0.5758 - val_accuracy: 0.6182 - val_loss: 0.6348 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8100 - loss: 0.4852 - val_accuracy: 0.6364 - val_loss: 0.6972 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8233 - loss: 0.4439 - val_accuracy: 0.6182 - val_loss: 0.7784 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8022 - loss: 0.4839 - val_accuracy: 0.6545 - val_loss: 0.7945 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8478 - loss: 0.3657 - val_accuracy: 0.6182 - val_loss: 0.8399 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8281 - loss: 0.4103 - val_accuracy: 0.5273 - val_loss: 0.9285 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8464 - loss: 0.3593 - val_accuracy: 0.6364 - val_loss: 0.9076 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8795 - loss: 0.3540 - val_accuracy: 0.5818 - val_loss: 0.9980 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9067 - loss: 0.3483 - val_accuracy: 0.5818 - val_loss: 1.0747 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9075 - loss: 0.2379 - val_accuracy: 0.5818 - val_loss: 1.1593 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9221 - loss: 0.2099 - val_accuracy: 0.6545 - val_loss: 1.1578 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 415ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.4853 - loss: 0.7239 - val_accuracy: 0.3818 - val_loss: 0.7812 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5095 - loss: 0.7092 - val_accuracy: 0.3455 - val_loss: 0.7603 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5929 - loss: 0.6688 - val_accuracy: 0.4364 - val_loss: 0.7274 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6532 - loss: 0.6536 - val_accuracy: 0.4545 - val_loss: 0.7491 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.7231 - loss: 0.6035 - val_accuracy: 0.4364 - val_loss: 0.8170 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.6954 - loss: 0.5867 - val_accuracy: 0.5091 - val_loss: 0.8252 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.8003 - loss: 0.5180 - val_accuracy: 0.5636 - val_loss: 0.7886 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8310 - loss: 0.4446 - val_accuracy: 0.5636 - val_loss: 0.8556 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8083 - loss: 0.4344 - val_accuracy: 0.4364 - val_loss: 0.9630 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7866 - loss: 0.5036 - val_accuracy: 0.5091 - val_loss: 0.8431 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8215 - loss: 0.4008 - val_accuracy: 0.5091 - val_loss: 0.9435 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8297 - loss: 0.3535 - val_accuracy: 0.6545 - val_loss: 0.7741 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8696 - loss: 0.3284 - val_accuracy: 0.5636 - val_loss: 0.9173 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8655 - loss: 0.3110 - val_accuracy: 0.5636 - val_loss: 1.0012 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.8860 - loss: 0.3003 - val_accuracy: 0.4727 - val_loss: 1.3663 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 634ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.4864 - loss: 0.7429 - val_accuracy: 0.3091 - val_loss: 0.7193 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.5101 - loss: 0.7132 - val_accuracy: 0.4000 - val_loss: 0.7136 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.5785 - loss: 0.6678 - val_accuracy: 0.4909 - val_loss: 0.7128 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.6826 - loss: 0.6201 - val_accuracy: 0.6545 - val_loss: 0.6505 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.6612 - loss: 0.6072 - val_accuracy: 0.6182 - val_loss: 0.6555 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7063 - loss: 0.5695 - val_accuracy: 0.6364 - val_loss: 0.6164 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8254 - loss: 0.4421 - val_accuracy: 0.6909 - val_loss: 0.6361 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8636 - loss: 0.3946 - val_accuracy: 0.6364 - val_loss: 0.6580 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8112 - loss: 0.4149 - val_accuracy: 0.6545 - val_loss: 0.7210 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8219 - loss: 0.4052 - val_accuracy: 0.6364 - val_loss: 0.7831 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8267 - loss: 0.4104 - val_accuracy: 0.5818 - val_loss: 0.7777 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8428 - loss: 0.3876 - val_accuracy: 0.6727 - val_loss: 0.6388 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.8879 - loss: 0.3343 - val_accuracy: 0.6727 - val_loss: 0.6918 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8678 - loss: 0.3650 - val_accuracy: 0.7091 - val_loss: 0.5755 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8530 - loss: 0.3804 - val_accuracy: 0.6909 - val_loss: 0.6981 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 94ms/step - accuracy: 0.5142 - loss: 0.7096 - val_accuracy: 0.5455 - val_loss: 0.6812 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.6123 - loss: 0.6770 - val_accuracy: 0.4727 - val_loss: 0.7216 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.6368 - loss: 0.6483 - val_accuracy: 0.5636 - val_loss: 0.6616 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.6408 - loss: 0.6360 - val_accuracy: 0.6000 - val_loss: 0.6846 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7078 - loss: 0.5838 - val_accuracy: 0.5818 - val_loss: 0.7909 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7810 - loss: 0.5480 - val_accuracy: 0.6364 - val_loss: 0.6970 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7990 - loss: 0.4335 - val_accuracy: 0.6909 - val_loss: 0.7455 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.8269 - loss: 0.4595 - val_accuracy: 0.6727 - val_loss: 0.7015 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.8059 - loss: 0.4625 - val_accuracy: 0.5455 - val_loss: 0.8578 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8595 - loss: 0.3767 - val_accuracy: 0.6545 - val_loss: 0.6770 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8216 - loss: 0.3909 - val_accuracy: 0.6364 - val_loss: 0.7425 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8700 - loss: 0.3335 - val_accuracy: 0.5818 - val_loss: 0.9537 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8735 - loss: 0.3064 - val_accuracy: 0.6545 - val_loss: 0.8178 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8647 - loss: 0.3323 - val_accuracy: 0.6182 - val_loss: 0.8296 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8765 - loss: 0.3505 - val_accuracy: 0.5818 - val_loss: 0.8649 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 417ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.4896 - loss: 0.7508 - val_accuracy: 0.3636 - val_loss: 0.7932 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.4946 - loss: 0.7042 - val_accuracy: 0.4000 - val_loss: 0.7275 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.5825 - loss: 0.6790 - val_accuracy: 0.4182 - val_loss: 0.7263 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.5838 - loss: 0.6676 - val_accuracy: 0.5273 - val_loss: 0.6712 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.6014 - loss: 0.6403 - val_accuracy: 0.6545 - val_loss: 0.6252 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.6406 - loss: 0.6161 - val_accuracy: 0.7273 - val_loss: 0.6072 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.6391 - loss: 0.6042 - val_accuracy: 0.6545 - val_loss: 0.6231 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.6950 - loss: 0.5752 - val_accuracy: 0.7091 - val_loss: 0.5856 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.7724 - loss: 0.5321 - val_accuracy: 0.6000 - val_loss: 0.7496 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7904 - loss: 0.5020 - val_accuracy: 0.7091 - val_loss: 0.6002 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.8045 - loss: 0.4746 - val_accuracy: 0.6364 - val_loss: 0.7343 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8323 - loss: 0.4105 - val_accuracy: 0.6727 - val_loss: 0.6531 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.8203 - loss: 0.4327 - val_accuracy: 0.6727 - val_loss: 0.6004 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.8188 - loss: 0.3896 - val_accuracy: 0.6364 - val_loss: 0.7542 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8453 - loss: 0.3794 - val_accuracy: 0.6909 - val_loss: 0.7066 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408ms/step\n",
            "\n",
            "=== Deep Learning on Agency_Label ===\n",
            "(275, 87, 40)\n",
            "(275, 87, 40)\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.5252 - loss: 0.7031 - val_accuracy: 0.5091 - val_loss: 0.7022 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.6441 - loss: 0.6703 - val_accuracy: 0.6364 - val_loss: 0.6798 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.5906 - loss: 0.6688 - val_accuracy: 0.5818 - val_loss: 0.6766 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.6853 - loss: 0.6111 - val_accuracy: 0.6364 - val_loss: 0.6546 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7114 - loss: 0.5941 - val_accuracy: 0.7273 - val_loss: 0.6148 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7449 - loss: 0.5643 - val_accuracy: 0.6545 - val_loss: 0.6395 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8049 - loss: 0.4724 - val_accuracy: 0.6909 - val_loss: 0.7086 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8513 - loss: 0.4265 - val_accuracy: 0.6182 - val_loss: 0.7054 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.7809 - loss: 0.4280 - val_accuracy: 0.6182 - val_loss: 0.7404 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.7966 - loss: 0.4749 - val_accuracy: 0.6000 - val_loss: 0.7751 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.8189 - loss: 0.4490 - val_accuracy: 0.6545 - val_loss: 0.7214 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8629 - loss: 0.4143 - val_accuracy: 0.6727 - val_loss: 0.7505 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8806 - loss: 0.3126 - val_accuracy: 0.6727 - val_loss: 0.8294 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8460 - loss: 0.3723 - val_accuracy: 0.5818 - val_loss: 0.8294 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8641 - loss: 0.3207 - val_accuracy: 0.6909 - val_loss: 0.7559 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 105ms/step - accuracy: 0.4357 - loss: 0.7372 - val_accuracy: 0.4909 - val_loss: 0.6899 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.5313 - loss: 0.6887 - val_accuracy: 0.5455 - val_loss: 0.6885 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.5863 - loss: 0.6830 - val_accuracy: 0.5455 - val_loss: 0.6876 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.5261 - loss: 0.6786 - val_accuracy: 0.6182 - val_loss: 0.6823 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.6786 - loss: 0.6562 - val_accuracy: 0.5455 - val_loss: 0.6638 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.6830 - loss: 0.6166 - val_accuracy: 0.6000 - val_loss: 0.6652 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.6383 - loss: 0.6478 - val_accuracy: 0.6545 - val_loss: 0.6555 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.6997 - loss: 0.5801 - val_accuracy: 0.6364 - val_loss: 0.6740 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.6857 - loss: 0.6007 - val_accuracy: 0.6364 - val_loss: 0.6364 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.7429 - loss: 0.5475 - val_accuracy: 0.6182 - val_loss: 0.6341 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.7536 - loss: 0.5145 - val_accuracy: 0.7455 - val_loss: 0.6039 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.8157 - loss: 0.4186 - val_accuracy: 0.6364 - val_loss: 0.8081 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8220 - loss: 0.4450 - val_accuracy: 0.7091 - val_loss: 0.6228 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8475 - loss: 0.4282 - val_accuracy: 0.6364 - val_loss: 0.7319 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7244 - loss: 0.5199 - val_accuracy: 0.5818 - val_loss: 0.7447 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 127ms/step - accuracy: 0.5705 - loss: 0.7001 - val_accuracy: 0.4545 - val_loss: 0.6923 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.5781 - loss: 0.6799 - val_accuracy: 0.5455 - val_loss: 0.6959 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.6829 - loss: 0.6539 - val_accuracy: 0.5091 - val_loss: 0.7304 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.7074 - loss: 0.6107 - val_accuracy: 0.5091 - val_loss: 0.7685 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.7409 - loss: 0.5931 - val_accuracy: 0.4909 - val_loss: 0.8297 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7553 - loss: 0.5124 - val_accuracy: 0.5818 - val_loss: 0.8948 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7785 - loss: 0.4873 - val_accuracy: 0.5455 - val_loss: 0.8818 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8320 - loss: 0.4600 - val_accuracy: 0.4909 - val_loss: 0.7930 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8206 - loss: 0.4146 - val_accuracy: 0.5273 - val_loss: 0.9122 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8444 - loss: 0.3991 - val_accuracy: 0.5273 - val_loss: 0.9179 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8863 - loss: 0.3426 - val_accuracy: 0.5091 - val_loss: 0.9721 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8398 - loss: 0.4071 - val_accuracy: 0.5818 - val_loss: 0.8911 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8650 - loss: 0.3986 - val_accuracy: 0.5636 - val_loss: 1.0034 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.9160 - loss: 0.3154 - val_accuracy: 0.6000 - val_loss: 0.9813 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.8266 - loss: 0.4133 - val_accuracy: 0.5455 - val_loss: 1.0775 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 414ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - accuracy: 0.5420 - loss: 0.7081 - val_accuracy: 0.5273 - val_loss: 0.6836 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.5104 - loss: 0.7191 - val_accuracy: 0.5818 - val_loss: 0.6804 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.5290 - loss: 0.6923 - val_accuracy: 0.5818 - val_loss: 0.6785 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.5376 - loss: 0.6682 - val_accuracy: 0.6545 - val_loss: 0.6443 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.6578 - loss: 0.6182 - val_accuracy: 0.6545 - val_loss: 0.6233 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.6864 - loss: 0.6137 - val_accuracy: 0.7091 - val_loss: 0.6178 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7037 - loss: 0.5851 - val_accuracy: 0.7091 - val_loss: 0.5678 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7733 - loss: 0.5537 - val_accuracy: 0.7455 - val_loss: 0.5795 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7360 - loss: 0.5862 - val_accuracy: 0.6364 - val_loss: 0.5931 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.7720 - loss: 0.5077 - val_accuracy: 0.7273 - val_loss: 0.5728 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.7703 - loss: 0.4922 - val_accuracy: 0.6909 - val_loss: 0.6028 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8335 - loss: 0.3906 - val_accuracy: 0.7273 - val_loss: 0.5954 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.8143 - loss: 0.4720 - val_accuracy: 0.7091 - val_loss: 0.6129 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8301 - loss: 0.4103 - val_accuracy: 0.6727 - val_loss: 0.5873 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7755 - loss: 0.4865 - val_accuracy: 0.7091 - val_loss: 0.6256 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 427ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 123ms/step - accuracy: 0.5046 - loss: 0.7503 - val_accuracy: 0.5273 - val_loss: 0.6997 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.5525 - loss: 0.7081 - val_accuracy: 0.5636 - val_loss: 0.6923 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.5105 - loss: 0.6996 - val_accuracy: 0.4727 - val_loss: 0.6969 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.5656 - loss: 0.6734 - val_accuracy: 0.6000 - val_loss: 0.6838 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.6208 - loss: 0.6568 - val_accuracy: 0.6364 - val_loss: 0.6783 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.6276 - loss: 0.6496 - val_accuracy: 0.5636 - val_loss: 0.6746 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.6154 - loss: 0.6470 - val_accuracy: 0.5455 - val_loss: 0.6630 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7724 - loss: 0.5729 - val_accuracy: 0.6364 - val_loss: 0.6602 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7118 - loss: 0.5581 - val_accuracy: 0.6182 - val_loss: 0.6705 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.6302 - loss: 0.6289 - val_accuracy: 0.4727 - val_loss: 0.6902 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7425 - loss: 0.5643 - val_accuracy: 0.6182 - val_loss: 0.6423 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7396 - loss: 0.5452 - val_accuracy: 0.5818 - val_loss: 0.6949 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7706 - loss: 0.5266 - val_accuracy: 0.5818 - val_loss: 0.7244 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7659 - loss: 0.4800 - val_accuracy: 0.5818 - val_loss: 0.6931 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7385 - loss: 0.4902 - val_accuracy: 0.5455 - val_loss: 0.7832 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408ms/step\n",
            "\n",
            "=== Deep Learning on Depression_Label ===\n",
            "(275, 87, 40)\n",
            "(275, 87, 40)\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 90ms/step - accuracy: 0.5220 - loss: 0.7646 - val_accuracy: 0.7636 - val_loss: 0.6283 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.5295 - loss: 0.7020 - val_accuracy: 0.7818 - val_loss: 0.5970 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.6740 - loss: 0.6269 - val_accuracy: 0.7455 - val_loss: 0.5868 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.6785 - loss: 0.6068 - val_accuracy: 0.7818 - val_loss: 0.4891 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7118 - loss: 0.5342 - val_accuracy: 0.7818 - val_loss: 0.4978 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7269 - loss: 0.5169 - val_accuracy: 0.7091 - val_loss: 0.5376 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.8022 - loss: 0.4581 - val_accuracy: 0.6545 - val_loss: 0.5416 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8348 - loss: 0.3923 - val_accuracy: 0.6727 - val_loss: 0.5900 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8566 - loss: 0.3608 - val_accuracy: 0.7273 - val_loss: 0.6021 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.8457 - loss: 0.3604 - val_accuracy: 0.6909 - val_loss: 0.6146 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8858 - loss: 0.3138 - val_accuracy: 0.6545 - val_loss: 0.6430 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8609 - loss: 0.3112 - val_accuracy: 0.7091 - val_loss: 0.6901 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9098 - loss: 0.2464 - val_accuracy: 0.6182 - val_loss: 0.7301 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8933 - loss: 0.2445 - val_accuracy: 0.6182 - val_loss: 0.7937 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8755 - loss: 0.2674 - val_accuracy: 0.6909 - val_loss: 0.6947 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 129ms/step - accuracy: 0.4767 - loss: 0.7766 - val_accuracy: 0.6909 - val_loss: 0.6601 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.5360 - loss: 0.6942 - val_accuracy: 0.4545 - val_loss: 0.6942 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.6087 - loss: 0.6604 - val_accuracy: 0.7636 - val_loss: 0.5672 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.6372 - loss: 0.6202 - val_accuracy: 0.7273 - val_loss: 0.5090 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7618 - loss: 0.5096 - val_accuracy: 0.6909 - val_loss: 0.5124 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8170 - loss: 0.4254 - val_accuracy: 0.7091 - val_loss: 0.5705 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8567 - loss: 0.3941 - val_accuracy: 0.7273 - val_loss: 0.5351 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.8525 - loss: 0.3425 - val_accuracy: 0.6182 - val_loss: 0.7464 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.8732 - loss: 0.3211 - val_accuracy: 0.7273 - val_loss: 0.8105 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.9001 - loss: 0.2675 - val_accuracy: 0.7091 - val_loss: 0.6093 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.9291 - loss: 0.2026 - val_accuracy: 0.7455 - val_loss: 0.5991 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8738 - loss: 0.2988 - val_accuracy: 0.7091 - val_loss: 0.7866 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.9102 - loss: 0.2296 - val_accuracy: 0.7636 - val_loss: 0.8034 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.9202 - loss: 0.2028 - val_accuracy: 0.7273 - val_loss: 0.8296 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9156 - loss: 0.2499 - val_accuracy: 0.6727 - val_loss: 0.8586 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 406ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 138ms/step - accuracy: 0.5061 - loss: 0.8283 - val_accuracy: 0.7273 - val_loss: 0.6531 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.5791 - loss: 0.6759 - val_accuracy: 0.5455 - val_loss: 0.6706 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.6454 - loss: 0.6467 - val_accuracy: 0.5091 - val_loss: 0.6950 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.6305 - loss: 0.6317 - val_accuracy: 0.5273 - val_loss: 0.6817 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.7234 - loss: 0.5548 - val_accuracy: 0.6000 - val_loss: 0.6576 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.7584 - loss: 0.5354 - val_accuracy: 0.6182 - val_loss: 0.7319 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8611 - loss: 0.4133 - val_accuracy: 0.6364 - val_loss: 0.8152 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8241 - loss: 0.3728 - val_accuracy: 0.5455 - val_loss: 0.9174 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.8636 - loss: 0.3733 - val_accuracy: 0.5273 - val_loss: 0.9516 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8804 - loss: 0.3534 - val_accuracy: 0.6000 - val_loss: 0.9803 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8819 - loss: 0.2862 - val_accuracy: 0.6182 - val_loss: 0.9046 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.9295 - loss: 0.2636 - val_accuracy: 0.6000 - val_loss: 0.9867 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9007 - loss: 0.2900 - val_accuracy: 0.6000 - val_loss: 0.9087 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.9046 - loss: 0.2520 - val_accuracy: 0.6182 - val_loss: 0.9558 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9019 - loss: 0.2572 - val_accuracy: 0.6364 - val_loss: 0.8194 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 133ms/step - accuracy: 0.5127 - loss: 0.7049 - val_accuracy: 0.6727 - val_loss: 0.6860 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.5458 - loss: 0.6872 - val_accuracy: 0.4364 - val_loss: 0.6946 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.6895 - loss: 0.6403 - val_accuracy: 0.4727 - val_loss: 0.7302 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8080 - loss: 0.5127 - val_accuracy: 0.5455 - val_loss: 0.8290 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7991 - loss: 0.4329 - val_accuracy: 0.5636 - val_loss: 0.8561 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.8245 - loss: 0.4251 - val_accuracy: 0.6000 - val_loss: 0.8177 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9081 - loss: 0.2899 - val_accuracy: 0.6000 - val_loss: 0.9030 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9151 - loss: 0.2521 - val_accuracy: 0.6909 - val_loss: 0.9047 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9234 - loss: 0.2297 - val_accuracy: 0.6364 - val_loss: 1.0057 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9288 - loss: 0.1801 - val_accuracy: 0.5818 - val_loss: 1.1064 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9241 - loss: 0.2601 - val_accuracy: 0.5818 - val_loss: 1.0242 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9617 - loss: 0.1558 - val_accuracy: 0.6364 - val_loss: 1.0068 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9418 - loss: 0.1710 - val_accuracy: 0.7091 - val_loss: 1.0574 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9313 - loss: 0.1582 - val_accuracy: 0.6545 - val_loss: 1.1884 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.9563 - loss: 0.1455 - val_accuracy: 0.6545 - val_loss: 1.2641 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.5378 - loss: 0.6883 - val_accuracy: 0.6727 - val_loss: 0.6726 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.5347 - loss: 0.6710 - val_accuracy: 0.7273 - val_loss: 0.5763 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.6010 - loss: 0.6528 - val_accuracy: 0.6727 - val_loss: 0.6074 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.7466 - loss: 0.5518 - val_accuracy: 0.6000 - val_loss: 0.6376 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.8290 - loss: 0.4537 - val_accuracy: 0.6727 - val_loss: 0.6745 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.8333 - loss: 0.4078 - val_accuracy: 0.6545 - val_loss: 0.8310 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8622 - loss: 0.3511 - val_accuracy: 0.6182 - val_loss: 0.8302 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8664 - loss: 0.3600 - val_accuracy: 0.5273 - val_loss: 1.1652 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8749 - loss: 0.3225 - val_accuracy: 0.4727 - val_loss: 1.4051 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8527 - loss: 0.3899 - val_accuracy: 0.5455 - val_loss: 1.1337 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9052 - loss: 0.2167 - val_accuracy: 0.6182 - val_loss: 1.1601 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9251 - loss: 0.2514 - val_accuracy: 0.6000 - val_loss: 1.1670 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9612 - loss: 0.1703 - val_accuracy: 0.6364 - val_loss: 1.1062 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.8979 - loss: 0.2712 - val_accuracy: 0.5818 - val_loss: 1.3139 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.9275 - loss: 0.1734 - val_accuracy: 0.5818 - val_loss: 1.3426 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411ms/step\n",
            "\n",
            "=== Deep Learning on PTSD_Label ===\n",
            "(275, 87, 40)\n",
            "(275, 87, 40)\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - accuracy: 0.4379 - loss: 0.8254 - val_accuracy: 0.3091 - val_loss: 0.7283 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.5929 - loss: 0.6787 - val_accuracy: 0.3091 - val_loss: 0.7944 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.6457 - loss: 0.6444 - val_accuracy: 0.5455 - val_loss: 0.6973 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.8019 - loss: 0.5208 - val_accuracy: 0.4727 - val_loss: 0.8637 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.7811 - loss: 0.4980 - val_accuracy: 0.6000 - val_loss: 0.7295 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8459 - loss: 0.4031 - val_accuracy: 0.6545 - val_loss: 0.6874 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8097 - loss: 0.4344 - val_accuracy: 0.6727 - val_loss: 0.7626 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8804 - loss: 0.3009 - val_accuracy: 0.8000 - val_loss: 0.5253 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8267 - loss: 0.3734 - val_accuracy: 0.7455 - val_loss: 0.5355 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.8677 - loss: 0.3317 - val_accuracy: 0.7636 - val_loss: 0.5215 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.9045 - loss: 0.2374 - val_accuracy: 0.7455 - val_loss: 0.5122 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9174 - loss: 0.2404 - val_accuracy: 0.7818 - val_loss: 0.5054 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9384 - loss: 0.1861 - val_accuracy: 0.8182 - val_loss: 0.4934 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9271 - loss: 0.1904 - val_accuracy: 0.8000 - val_loss: 0.5783 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9123 - loss: 0.2318 - val_accuracy: 0.7273 - val_loss: 0.6401 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 426ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.4788 - loss: 0.8167 - val_accuracy: 0.5455 - val_loss: 0.6857 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.6333 - loss: 0.6348 - val_accuracy: 0.5818 - val_loss: 0.6969 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.7460 - loss: 0.5750 - val_accuracy: 0.6545 - val_loss: 0.6559 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.7770 - loss: 0.5449 - val_accuracy: 0.6182 - val_loss: 0.6616 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7843 - loss: 0.5122 - val_accuracy: 0.7091 - val_loss: 0.5811 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.8290 - loss: 0.4355 - val_accuracy: 0.7818 - val_loss: 0.6028 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8470 - loss: 0.3747 - val_accuracy: 0.7636 - val_loss: 0.5767 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.8622 - loss: 0.3559 - val_accuracy: 0.7636 - val_loss: 0.6408 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.8507 - loss: 0.3612 - val_accuracy: 0.7091 - val_loss: 0.6773 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8933 - loss: 0.3041 - val_accuracy: 0.7818 - val_loss: 0.5984 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.8791 - loss: 0.3256 - val_accuracy: 0.7818 - val_loss: 0.6412 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.9085 - loss: 0.3143 - val_accuracy: 0.8000 - val_loss: 0.6538 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.8908 - loss: 0.2758 - val_accuracy: 0.6909 - val_loss: 0.7024 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.8975 - loss: 0.2793 - val_accuracy: 0.7636 - val_loss: 0.7792 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8699 - loss: 0.3406 - val_accuracy: 0.7818 - val_loss: 0.6772 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.4732 - loss: 0.7576 - val_accuracy: 0.6727 - val_loss: 0.6110 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.6644 - loss: 0.6290 - val_accuracy: 0.6182 - val_loss: 0.5681 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7826 - loss: 0.5298 - val_accuracy: 0.6727 - val_loss: 0.5893 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.7985 - loss: 0.4937 - val_accuracy: 0.6727 - val_loss: 0.6027 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8468 - loss: 0.4132 - val_accuracy: 0.6364 - val_loss: 0.7513 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8650 - loss: 0.3497 - val_accuracy: 0.6545 - val_loss: 0.7600 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.8803 - loss: 0.3260 - val_accuracy: 0.7091 - val_loss: 0.6939 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.8886 - loss: 0.3189 - val_accuracy: 0.6545 - val_loss: 0.7933 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.8968 - loss: 0.2926 - val_accuracy: 0.6909 - val_loss: 0.7909 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9280 - loss: 0.3170 - val_accuracy: 0.6364 - val_loss: 0.7806 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9183 - loss: 0.2420 - val_accuracy: 0.6545 - val_loss: 0.8216 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - accuracy: 0.9192 - loss: 0.2407 - val_accuracy: 0.6727 - val_loss: 0.8054 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9121 - loss: 0.2462 - val_accuracy: 0.6545 - val_loss: 0.8583 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9210 - loss: 0.2309 - val_accuracy: 0.6000 - val_loss: 0.8937 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.8606 - loss: 0.3273 - val_accuracy: 0.6545 - val_loss: 0.9395 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407ms/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 101ms/step - accuracy: 0.4614 - loss: 0.7092 - val_accuracy: 0.6364 - val_loss: 0.6879 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.4800 - loss: 0.6931 - val_accuracy: 0.6909 - val_loss: 0.6818 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.5306 - loss: 0.6882 - val_accuracy: 0.6727 - val_loss: 0.6650 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.6153 - loss: 0.6685 - val_accuracy: 0.7636 - val_loss: 0.6167 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.6981 - loss: 0.6023 - val_accuracy: 0.5818 - val_loss: 0.6408 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.6482 - loss: 0.6007 - val_accuracy: 0.6909 - val_loss: 0.5854 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.8034 - loss: 0.4915 - val_accuracy: 0.7636 - val_loss: 0.5398 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8498 - loss: 0.4116 - val_accuracy: 0.7091 - val_loss: 0.6680 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.8331 - loss: 0.4014 - val_accuracy: 0.6545 - val_loss: 0.6366 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.8374 - loss: 0.3932 - val_accuracy: 0.7091 - val_loss: 0.7341 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.8854 - loss: 0.3305 - val_accuracy: 0.7455 - val_loss: 0.6761 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8736 - loss: 0.3363 - val_accuracy: 0.6364 - val_loss: 0.9622 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.8963 - loss: 0.2871 - val_accuracy: 0.6364 - val_loss: 0.9736 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.8871 - loss: 0.3163 - val_accuracy: 0.7273 - val_loss: 0.7155 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.9280 - loss: 0.2787 - val_accuracy: 0.7273 - val_loss: 0.7617 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4s/step\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 1/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 146ms/step - accuracy: 0.5406 - loss: 0.7441 - val_accuracy: 0.3455 - val_loss: 0.7501 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 2/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.5815 - loss: 0.6693 - val_accuracy: 0.6727 - val_loss: 0.6556 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 3/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.7036 - loss: 0.6214 - val_accuracy: 0.7091 - val_loss: 0.6366 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 4/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.7010 - loss: 0.5967 - val_accuracy: 0.6909 - val_loss: 0.6453 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 5/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.7383 - loss: 0.5372 - val_accuracy: 0.5636 - val_loss: 0.7572 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 6/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8387 - loss: 0.4384 - val_accuracy: 0.5636 - val_loss: 0.7397 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 7/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.8463 - loss: 0.4127 - val_accuracy: 0.7091 - val_loss: 0.6014 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 8/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.8349 - loss: 0.3880 - val_accuracy: 0.7273 - val_loss: 0.5852 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 9/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8588 - loss: 0.3365 - val_accuracy: 0.6727 - val_loss: 0.6703 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 10/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.8631 - loss: 0.3141 - val_accuracy: 0.6545 - val_loss: 0.7042 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 11/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8850 - loss: 0.3321 - val_accuracy: 0.6727 - val_loss: 0.6483 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 12/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8667 - loss: 0.3227 - val_accuracy: 0.7273 - val_loss: 0.6319 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 13/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8887 - loss: 0.2650 - val_accuracy: 0.7273 - val_loss: 0.7018 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 14/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9119 - loss: 0.2603 - val_accuracy: 0.7636 - val_loss: 0.7489 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 15/15\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.8941 - loss: 0.2523 - val_accuracy: 0.7091 - val_loss: 0.7442 - learning_rate: 0.0100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics_dl(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    sens = tp / (tp + fn) if (tp + fn) else 0\n",
        "    spec = tn / (tn + fp) if (tn + fp) else 0\n",
        "    return acc, sens, spec  # keep raw (not rounded yet)\n",
        "\n",
        "# Store mean  std dev metrics\n",
        "results_dl = {}\n",
        "\n",
        "df = pd.read_csv('/content/GroundTruth Table.csv')\n",
        "MAX_CHUNKS = 87\n",
        "labels = ['Appetite_Label', 'Anxiety_Label', 'Sleep_Label', 'Agency_Label', 'Depression_Label', 'PTSD_Label']\n",
        "\n",
        "for label in labels:\n",
        "    print(f\"\\n=== Deep Learning on {label} ===\")\n",
        "    X = summary_features.copy()\n",
        "    y = df[label]\n",
        "\n",
        "    discrete_features = [False] * X.shape[1]\n",
        "    mi_scores = make_mi_scores(X, y, discrete_features)\n",
        "    filtered_scores = mi_scores[mi_scores > 0]\n",
        "    filtered_X = X[filtered_scores.index]\n",
        "\n",
        "    # Drop highly correlated features\n",
        "    corr_matrix = filtered_X.corr().abs()\n",
        "    to_drop = {\n",
        "        col2 if filtered_scores[col1] >= filtered_scores[col2] else col1\n",
        "        for i, col1 in enumerate(corr_matrix.columns)\n",
        "        for j, col2 in enumerate(corr_matrix.columns)\n",
        "        if i < j and corr_matrix.loc[col1, col2] > 0.8\n",
        "    }\n",
        "\n",
        "    final_X = filtered_X.drop(columns=to_drop)\n",
        "    final_scores = filtered_scores.drop(labels=to_drop)\n",
        "\n",
        "    # Select top 20% features\n",
        "    top_features = final_scores.sort_values(ascending=False).head(int(len(final_scores) * 0.2))\n",
        "    top_features_list = top_features.index.tolist()\n",
        "\n",
        "    # Map feature names to All_features.csv naming\n",
        "    extracted_features = []\n",
        "    for feature in top_features_list:\n",
        "        match = re.match(r'^(MFCC|eGeMAPS|FAU)_f(\\d+)_((?:mean)|(?:std)|(?:min)|(?:max))$', feature)\n",
        "        if match:\n",
        "            category, number, _ = match.groups()\n",
        "            number = int(number)\n",
        "            if category == \"MFCC\":\n",
        "                mapped = f\"feature_{number-1}\"\n",
        "            elif category == \"eGeMAPS\":\n",
        "                mapped = f\"feature_{number + 99}\"\n",
        "            elif category == \"FAU\":\n",
        "                mapped = f\"feature_{number + 199}\"\n",
        "            extracted_features.append(mapped)\n",
        "\n",
        "    # Load extracted features\n",
        "    X_all_label = pd.read_csv('/content/All_features.csv', usecols=extracted_features)\n",
        "    y_all_label = df[label]\n",
        "\n",
        "    X_all_label = np.array(X_all_label).reshape(275, MAX_CHUNKS, -1)\n",
        "    X_all_label = X_all_label[:, :-1, :]\n",
        "    y_all_label = np.array(y_all_label)\n",
        "\n",
        "    # Store all metrics across 10 runs\n",
        "    all_runs_metrics = []\n",
        "\n",
        "    for run in range(10):\n",
        "        fold_metrics = []\n",
        "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + run)\n",
        "\n",
        "        for train_index, test_index in kf.split(X_all_label, y_all_label):\n",
        "            X_train, X_test = X_all_label[train_index], X_all_label[test_index]\n",
        "            y_train, y_test = y_all_label[train_index], y_all_label[test_index]\n",
        "\n",
        "            model = build_model(input_shape=(X_all_label.shape[1], X_all_label.shape[2]))\n",
        "            model.fit(X_train, y_train, batch_size=32, epochs=15, verbose=0)\n",
        "\n",
        "            y_pred_prob = model.predict(X_test)\n",
        "            y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "            acc, sens, spec = compute_metrics_dl(y_test, y_pred)\n",
        "            fold_metrics.append((acc, sens, spec))\n",
        "\n",
        "        all_runs_metrics.append(np.mean(fold_metrics, axis=0))\n",
        "\n",
        "    all_runs_metrics = np.array(all_runs_metrics)\n",
        "    means = np.mean(all_runs_metrics, axis=0)\n",
        "    stds = np.std(all_runs_metrics, axis=0)\n",
        "\n",
        "    results_dl[label] = {\n",
        "        \"accuracy\": f\"{means[0]:.4f}  {stds[0]:.4f}\",\n",
        "        \"sensitivity\": f\"{means[1]:.4f}  {stds[1]:.4f}\",\n",
        "        \"specificity\": f\"{means[2]:.4f}  {stds[2]:.4f}\"\n",
        "    }\n",
        "\n",
        "# Convert to DataFrame and save\n",
        "summary_df = pd.DataFrame(results_dl).T\n",
        "summary_df.index.name = \"Label\"\n",
        "\n",
        "print(\"\\n===== Final Deep Learning Metrics (10 runs, mean  std) =====\")\n",
        "print(summary_df)\n",
        "\n",
        "summary_df.to_csv(\"dl_summary_metrics.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hneg4t-HrKp7",
        "outputId": "e6d9d7d5-5b77-4bae-9172-109d5d9e6595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Deep Learning on Appetite_Label ===\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 433ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 443ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 422ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 420ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 424ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 429ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 433ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 501ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 443ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 477ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 439ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 426ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 439ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 462ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 425ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411ms/step\n",
            "\n",
            "=== Deep Learning on Anxiety_Label ===\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 426ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 506ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 427ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 559ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 414ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 444ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 659ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 406ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 418ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 410ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 424ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 434ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 463ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 446ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 472ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 511ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 647ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 464ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 485ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 416ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 417ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 564ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 496ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 443ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437ms/step\n",
            "\n",
            "=== Deep Learning on Sleep_Label ===\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 416ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 433ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 472ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 465ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 400ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 434ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 499ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 425ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 429ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 543ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 432ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 662ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 448ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 427ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 417ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 424ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 465ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 492ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 531ms/step\n",
            "\n",
            "=== Deep Learning on Agency_Label ===\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 410ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 413ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 424ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 474ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 462ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 449ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 474ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 444ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 434ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 429ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 655ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 416ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 428ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 428ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 682ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 444ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 428ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 427ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 480ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 620ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 421ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 434ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 509ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 486ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483ms/step\n",
            "\n",
            "=== Deep Learning on Depression_Label ===\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 446ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 472ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 682ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 517ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 441ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 665ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 454ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 427ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 414ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 433ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 421ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 516ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 419ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 424ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 420ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 487ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 417ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 416ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 681ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 517ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 500ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 446ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438ms/step\n",
            "\n",
            "=== Deep Learning on PTSD_Label ===\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 426ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 420ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 415ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 443ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 413ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 422ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame from the results dictionary\n",
        "dl_df = pd.DataFrame.from_dict(\n",
        "    results_dl,\n",
        "    orient='index',\n",
        "    columns=['Accuracy', 'Sensitivity', 'Specificity']\n",
        ")\n",
        "\n",
        "# Add model name as a column\n",
        "dl_df.insert(0, 'Model', 'Deep CNN-BiLSTM')\n",
        "\n",
        "# Reorder index (labels become columns)\n",
        "dl_df = dl_df.transpose()\n",
        "dl_df.columns = [f'{label}' for label in dl_df.columns]\n",
        "\n",
        "# Optional: flatten the multi-index style\n",
        "dl_df.reset_index(inplace=True)\n",
        "dl_df.rename(columns={'index': 'Metric'}, inplace=True)\n",
        "\n",
        "# Save to CSV\n",
        "dl_df.to_csv(\"deep_model_results.csv\", index=False)\n",
        "\n",
        "# Display it\n",
        "print(\"\\nSaved Deep Learning metrics to 'deep_model_results.csv':\\n\")\n",
        "print(dl_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-PgByT0jHw8",
        "outputId": "04a10ad2-3dd2-4761-a82a-29a053778456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved Deep Learning metrics to 'deep_model_results.csv':\n",
            "\n",
            "        Metric   Appetite_Label    Anxiety_Label      Sleep_Label  \\\n",
            "0        Model  Deep CNN-BiLSTM  Deep CNN-BiLSTM  Deep CNN-BiLSTM   \n",
            "1     Accuracy           0.6582           0.6582           0.6182   \n",
            "2  Sensitivity             0.48            0.669           0.5632   \n",
            "3  Specificity           0.7267           0.6455           0.6467   \n",
            "\n",
            "      Agency_Label Depression_Label       PTSD_Label  \n",
            "0  Deep CNN-BiLSTM  Deep CNN-BiLSTM  Deep CNN-BiLSTM  \n",
            "1           0.6146           0.6473             0.72  \n",
            "2           0.6598           0.3791           0.4013  \n",
            "3           0.5693           0.7317           0.8669  \n"
          ]
        }
      ]
    }
  ]
}